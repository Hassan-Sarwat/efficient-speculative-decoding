# Efficient Speculative Decoding: Chain of Draft vs. Chain of Thought

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![Status](https://img.shields.io/badge/status-wip-orange)

This project explores the efficiency of **Speculative Decoding** by comparing standard **Chain of Thought (CoT)** reasoning against a token-optimized **Chain of Draft (CoD)** approach.

We train and distill knowledge from a "Teacher" model (Qwen 2.5 14B) into a smaller "Draft" model (Qwen 2.5 0.5B) across three difficulty scenarios (Easy, Medium, Hard) to measure alignment, speedups, and token efficiency.

## ğŸš€ Key Features
* **Pipeline Automation**: End-to-end training, distillation, and draft model fine-tuning in a single script.
* **Methodology Comparison**: Direct comparison between CoT (verbose) and CoD (concise) reasoning styles.
* **Robust Environment**: Uses `uv` for fast, conflict-free dependency management (Unsloth for training, vLLM for serving).
* **Experiment Queuing**: Scripts included to run extensive nightly benchmarks.

## ğŸ› ï¸ Installation & Setup

We use **uv** to manage our environments (`env_train` for Unsloth, `env_serve` for vLLM).

### 1. Hardware Requirements
* **GPU**: RTX 3090 / 4090 (24GB VRAM) or A40/A100.
* **Disk**: >100GB (for checkpoints and datasets).

### 2. Setup Environments
Run the setup script to install `uv` and create the required virtual environments:

```bash
bash scripts/uv_setup_envs.sh
```

## ğŸƒâ€â™‚ï¸ Usage

### 1. Run a Specific Experiment

The main entry point is `scripts/train_pipeline.sh`. It handles:

1. **Train Target (14B)**: Fine-tunes the base model on the scenario data.
2. **Distill Data**: Generates a synthetic dataset using the trained Target model.
3. **Train Draft (0.5B)**: Fine-tunes the draft model on the distilled data.

**Syntax:**

```bash
bash scripts/train_pipeline.sh -t <type> -s <scenario>
```

**Arguments:**

* `-t`: Type of reasoning. Options: `cot` (Chain of Thought), `cod` (Chain of Draft).
* `-s`: Difficulty scenario. Options: `easy` (GSM8K), `medium` (MATH Lvl 1-2), `hard` (MATH Lvl 3-4).

**Example:**

```bash
# Run Chain of Draft on the Hard scenario
bash scripts/train_pipeline.sh -t cod -s hard
```

### 2. Run the "Nightly Queue"

To run all experiments (CoT & CoD across all difficulties) sequentially (e.g., while sleeping):

```bash
# Creates a log file and runs all jobs in sequence
bash scripts/run_queue.sh > nightly_log.txt 2>&1
```

### 3. Untrained Baseline

To generate baselines using an untrained vanilla model:

```bash
bash scripts/untrained_pipeline.sh -s <scenario>

```

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ configs/                  # YAML configs (Base templates)
â”‚   â”œâ”€â”€ target_14b.yaml       # Teacher config (Qwen 2.5 14B)
â”‚   â””â”€â”€ draft_0-5b.yaml       # Student config (Qwen 2.5 0.5B)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/            # Cleaned training data (Input)
â”‚   â”œâ”€â”€ distilled/            # Synthetic data generated by Teacher
â”‚   â””â”€â”€ training/             # (Legacy/Backup data folders)
â”œâ”€â”€ data_generation/          # Scripts for initial dataset creation & cleaning
â”‚   â””â”€â”€ README.md             # ğŸ“– Full documentation for data generation process
â”œâ”€â”€ models/                   # Final saved LoRA adapters
â”‚   â”œâ”€â”€ target_cot_easy_14b/
â”‚   â””â”€â”€ draft_cot_easy_0.5b/
â”œâ”€â”€ checkpoints/              # Intermediate training checkpoints (Safe to delete)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_pipeline.sh     # Main experiment runner
â”‚   â”œâ”€â”€ run_queue.sh          # Batch runner for all experiments
â”‚   â”œâ”€â”€ distill_untrained.py  # Baseline generation script
â”‚   â””â”€â”€ uv_setup_envs.sh      # Environment installation
â”œâ”€â”€ train.py                  # Universal SFTTrainer script (Unsloth)
â”œâ”€â”€ distill_data.py           # Data generation script (vLLM)
â””â”€â”€ benchmark.py              # Speculative Decoding Benchmark (vLLM)

```

## ğŸ“Š Experiments & Data

The project runs experiments on three distinct dataset splits:

* **Easy**: GSM8K (Grade school math).
* **Medium**: MATH Dataset (Levels 1-2).
* **Hard**: MATH Dataset (Levels 3-4).

For each split, we compare the **Match Rate** (Acceptance Rate) of the draft model when acting as a speculative decoder for the target model.

> **Note**: The datasets were generated using custom scripts. For a deep dive into the generation, cleaning, and analysis process, please refer to the dedicated **[Data Generation README](https://www.google.com/search?q=data_generation/README.md)**.

