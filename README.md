# Efficient Speculative Decoding: Chain of Draft vs. Chain of Thought

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![Status](https://img.shields.io/badge/status-wip-orange)

This project explores the efficiency of **Speculative Decoding** by comparing standard **Chain of Thought (CoT)** reasoning against a token-optimized **Chain of Draft (CoD)** approach.

## Key Features
* **Pipeline Automation**: End-to-end training and benchmarking.
* **Methodology Comparison**: Direct comparison between CoT (verbose) and CoD (concise) reasoning styles.
* **Robust Environment**: Uses `uv` for dependency management.

## Installation & Setup

We use **uv** to manage our environments (`env_train` for Unsloth, `env_serve` for vLLM).

### Setup Environments
```bash
bash scripts/uv_setup_envs.sh
```

## Usage

### 1. Train Models
Run the training pipeline to fine-tune the Target (14B) and Draft (0.5B) models. This script does NOT merge the models; it only saves the LoRA adapters.

```bash
bash scripts/train_pipeline.sh -t <type> -s <scenario>
```
*   `-t`: `cot` or `cod`
*   `-s`: `easy` (GSM8K), `medium` (MATH Lvl 1-2), `hard` (MATH Lvl 3-4)

### 2. Benchmark
Run the benchmark pipeline. This script handles ephemeral merging (Merge -> Benchmark -> Delete) to save space.

```bash
bash scripts/benchmark_pipeline.sh -t <type> -s <scenario>
```

### 3. Untrained Baseline
Generate baselines using an untrained vanilla model.

```bash
bash scripts/untrained_pipeline.sh <scenario>
```

### 4. Run Nightly Queue
Run all experiments sequentially.

```bash
bash scripts/run_queue.sh > nightly_log.txt 2>&1
```

## Project Structure

```plaintext
.
├── configs/                  # YAML configs
├── data/
│   ├── processed/            # Cleaned training data
│   └── distilled/            # Synthetic data generated by Teacher
├── data_generation/          # Scripts for initial dataset creation
├── models/                   # Final saved LoRA adapters
├── scripts/
│   ├── train_pipeline.sh     # Training pipeline (No merge)
│   ├── benchmark_pipeline.sh # Benchmark pipeline (Ephemeral merge)
│   ├── untrained_pipeline.sh # Baseline generation
│   └── uv_setup_envs.sh      # Environment setup
├── src/
│   ├── train.py              # SFTTrainer script
│   ├── distill_data.py       # Data generation script
│   └── merge_adapter.py      # Adapter merging script
└── tests/
    └── benchmark.py          # Speculative Decoding Benchmark
```

## Data
*   **Easy**: GSM8K
*   **Medium**: MATH (Levels 1-2)
*   **Hard**: MATH (Levels 3-4)

For data generation details, see `data_generation/README.md`.
